---
layout: post
title: scrapy 
tags: [scrapy,python,spider]
---
### scrapy
#### 简介
[a fast high-level screen scraping and web crawling framework for Python](https://github.com/scrapy/scrapy)
#### 安装

```bash
pip install scrapy
``` 

#### 创建项目

```bash
scrapy startproject
```
<img src="/static/images/scrapy.jpg" style="height:200px;width:300px;"/>

#### 基本概念
*   item:数据结构
*   spider:定义爬虫,抓取网页,并提取数据结构
*   item pipeline:保存数据结构
#### item
爬取的主要目标就是从非结构性的数据源提取结构性数据
Item 对象是种简单的容器，保存了爬取到得数据

```python
class ZhiHuItem(scrapy.Item):
    db_name = 'dw_crawl_zhi_hu'
    header = scrapy.Field()
    link = scrapy.Field()
    content = scrapy.Field()
    gmt_create = scrapy.Field()
    gmt_modified = scrapy.Field()

    def __init__(self, *args, **kwargs):
        super(ZhiHuItem, self).__init__(*args, **kwargs)
        gmt = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        self['gmt_create'] = gmt
        self['gmt_modified'] = gmt
```

#### spider
Spider类定义了如何爬取某个(或某些)网站
提取item

```python
class ZhiHuSpider(scrapy.Spider):
    name = 'zhihu_spider'
    start_urls = [
        'http://www.zhihu.com/explore'
    ]
    domain = 'http://www.zhihu.com'

    def parse(self, response):
        item_list = response.xpath('//div[@data-type="daily"]/div')
        for item in item_list:
            header = item.xpath('./h2/a/text()').extract()[0].strip()
            link = item.xpath('./h2/a/@href').extract()[0]
            content = item.xpath('./div/div[@class="zm-item-rich-text"]/textarea/text()').extract()[0].strip()
            zhi_hu_item = items.ZhiHuItem()
            zhi_hu_item['header'] = header
            zhi_hu_item['link'] = link
            zhi_hu_item['content'] = content
            yield zhi_hu_item
```

#### item pipeline
保存爬取结果

```python
class DbPipeline(object):
    def open_spider(self, spider):
        self.db_pool = adbapi.ConnectionPool('MySQLdb',
                                             db='share',
                                             user='root',
                                             host='127.0.0.1',
                                             charset='utf8',
        )

    def process_item(self, item, spider):
        if isinstance(item, ZhiHuItem):
            sql = 'insert into ' + item.db_name + '(' + ','.join(item.keys()) + ')values(' + ','.join(
                ['"' + i.encode('utf8').replace('"', '') + '"' for i in item.values()]) + ')'
            self.db_pool.runQuery(sql)
        return item

    def close_spider(self, spider):
        self.db_pool.close()
```